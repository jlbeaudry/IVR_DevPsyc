---
title: "Immersive Virtual Reality Implementations in Developmental Psychology"
author: "Araiza-Alba, Keane, Beaudry, Kaufman (Code by Jennifer Beaudry)"
date: '`r format (Sys.time(), "%d %B %Y")`'
output: 
  pdf_document:
        number_sections: true
        dev: cairo_pdf
csl: apa-old-doi-prefix.csl
bibliography: Beaudry_Library.bib
---
```{r setup, include=FALSE}
options(scipen = 999)

## Global options
knitr::opts_chunk$set(
  echo = FALSE,
  include = FALSE,
  cache = FALSE, # do not use this with github!
  prompt = FALSE,
  tidy = TRUE,
  comment = NA,
  message = FALSE,
  warning = TRUE,
  results = 'asis',
  fig.showtext = TRUE, 
  fig.align = "center", 
  fig.height = 6,
  fig.width = 12
)
knitr::opts_knit$set(width = 75)

```

```{r library}
# load the library
  # need to review the library for the IVR paper
library(tinytex)
library(knitr)
library(here)
library(tidyverse)
library(compute.es) # for Cohen's d
library(BSDA) # run t-test from summary data
library(fmsb) # for odds ratio [currently not using because we are using Cohen's h]
library(pwr) # for Cohen's h
library(finalfit) # to keep trailing zeros
library(purrrlyr) # keep if I'm using by_row
library(apa) # report apa style results in rmarkdown
library(effsize) # Cohen's d for within subjects data
library(kableExtra) # build tables
library(effectsize) # partial eta2 from F stats


# library(captioner) 
# library(plyr)
# library(extrafont) #load the fonts needed for latex font in ggplot
# library(skimr)
# library(ggbeeswarm)
#library(ggrepel) use this if I'm going to add labels
# library(plotly)
# library(RColorBrewer)
# library(datapasta) # I don't need this to run the file, so delete when I'm done.


```

```{r style}

# standard kable settings

kable_default <- function(dat, col.names, caption) {
  dat %>% 
    knitr::kable(booktabs = T, linesep = "",
            col.names = colnames,
            caption = title,
              align = 'lcc')
}

#### KABLE STYLE #####  
  # use different styles depending on the output#

styler_html <- function(dat) {
  dat %>%
     kable_styling(bootstrap_options = "striped", full_width = F, 
                 position = 'center',
                font_size = 16)
}

styler_pdf <- function(dat) {
  dat %>%
     kable_styling(full_width = F, 
                latex_options = c("striped", "HOLD_position"))
}

#### CAPTIONER TO ADD FIGURE CAPTIONS ###

# fig_nums <- captioner(prefix = "Figure")

### USE LATEX FONT FOR GGPLOT, PLUS CHANGE FONT SIZE
  # 18 FOR PDF; 12 FOR HTML

 theme_pdf <- theme_classic(base_size = 18, base_family = "LM Roman 10")
 theme_html <-theme_classic(base_size = 12)
 
```

```{r load data}

# IMPORT DATA SETS FOR THE INDIVIDUAL STUDIES

# Hua et al
df_hua <- read_csv (here::here("data_input", "hua_data_input.csv")) %>% 
  select(-c(pages, section, notes, paper, table))

# Jeffs et al
# this doesn't work, so don't load it now
# df_jeffs <- read_csv (here::here("data_input", "jeffs_data_input.csv"))

# Kipping et al
df_kip <- read_csv (here::here("data_input", "kipping_data_input.csv")) %>% 
  select (-c(pages, section, notes, paper, table))

# Sil et al
df_sil <- read_csv (here::here("data_input", "sil_data_input.csv")) %>% 
  select (-c(pages, section, notes, paper, table))

# Dahlquist et al (2010)
df_dahl <- read_csv (here::here("data_input", "dahlquist_2010_data_input.csv")) %>% 
  select (-c(pages, section, notes, paper, table))

# Dahlquist et al (2009)
df_dahl_09 <- read_csv (here::here("data_input", "dahlquist_2009_data_input.csv")) %>% 
  select (-c(pages, section, notes, paper, table))

# Negut et al (2016)
df_negut <- read_csv (here::here("data_input", "negut_data_input.csv")) %>% 
  select (-c(pages, section, notes, paper, table))

# Pollak et al (2009)
df_pol <- read_csv (here::here("data_input", "pollak_data_input.csv")) %>% 
  select (-c(pages, section, notes, paper, table))

# Bioulac et al (2012)
df_bio <- read_csv (here::here("data_input", "bioulac_data_input.csv")) %>% 
  select (-c(pages, section, notes, paper, table))
```

For each data set, I will provide the output in this pdf and also in a `.csv` file. 
You can easily copy and paste the numbers from the `.csv` file into a word table.

```{r functions}

# Cohen's h with confidence intervals
# from: https://stats.stackexchange.com/questions/80082/confidence-intervals-for-cohens-h-effect-size/409927

cohens_h = function( prop_1, prop_2, n1, n2, ci = 0.95 ){
    x1 = asin(sign(prop_1) * sqrt(abs(prop_1)))
    x2 = asin(sign(prop_2) * sqrt(abs(prop_2)))
    es = x1 - x2
    se = sqrt(0.25 * (1 / n1 + 1 / n2 ))
    ci_diff = qnorm(1 - (1-ci) / 2) * se
    return( c( h = es*2, h_low = (es-ci_diff)*2, h_upp = (es+ci_diff)*2 ) )
}

# Vargha-Delaney;s A function

# Using the Mann-Whitney U values reported on p. 517, I was able to calculate 
 #the effect size with the Vargha-Delaney's A (using formula 7 from Ruscio, 2008).

  # A = (n1n2-U)/(n1n2), where U = Mann-Whitney U value

VD.A.sum <- function (U, n1, n2) {
  A <- ((n1*n2-U)/(n1*n2)) 
    print(round(A,2))
  }

```


```{r functions_I_can_delete, eval = FALSE}

# I NEED TO CLEAN THE REST OF THIS UP!
# pull out the relevant info

vr_ctrl <- function (x) {c(
  (m.1 = x$vr_m), 
  (m.2 = x$ctrl_m),
  (sd.1 = x$vr_sd), 
  (sd.2 = x$ctrl_sd), 
  (n.1 = x$vr_n), 
  (n.2 = x$ctrl_n)
  )
}

vr_ctrl(df_hua) #this works..., but not row by row....and it doesn't retain the 
  # column names, so it's kind of useless...

df_hua %>% by_row(mes)


p <- split(df_hua, df$paper) #this will be useful EXCEPT that I don't want to apply
  # this to lists, I want to apply it to the data frame

measure <- split(df_hua, df$measure)

dat(measure)


# DON"T TOUCH THESE
# effect size calculations from mean & sd [these both work, but they aren't pretty]

# for control > VR

mes(m.1 = ctrl_m, m.2 = vr_m, 
          sd.1 = ctrl_sd, sd.2 = vr_sd, 
          n.1 = ctrl_n, n.2 = vr_n,
          level = 95, dig = 4, verbose = TRUE, id=1, data=df)



```

```{r playing around, eval = FALSE}

# I'm sure I can delete this...

# TRYING TO DO SOMETHING WITH PURR

# this might be useful: http://www.rebeccabarter.com/blog/2019-08-19_purrr/

df_hua %>% 
  pwalk(function(...) {
    current <- tibble(...)
    print(current$d)
  })

# for control > VR

d1 <- mes(m.1 = ctrl_m, m.2 = vr_m, 
          sd.1 = ctrl_sd, sd.2 = vr_sd, 
          n.1 = ctrl_n, n.2 = vr_n,
          level = 95, dig = 4, verbose = TRUE, id=1, data=df)


df <- df_hua %>% 
  filter(measure == "pain_before")

# THIS WORKS

d1 <- mes(m.1 = ctrl_m, m.2 = vr_m, 
          sd.1 = ctrl_sd, sd.2 = vr_sd, 
          n.1 = ctrl_n, n.2 = vr_n,
          level = 95, dig = 4, verbose = TRUE, id=1, data=df)

df <- df_hua 

lapply(mes(m.1 = df$ctrl_m, m.2 = df$vr_m, sd.1 = df$ctrl_sd, sd.2 = df$vr_sd, n.1 = df$ctrl_n, 
    n.2 = df$vr_n,level = 95, dig = 4, verbose = TRUE, id=NULL, data=NULL))

lapply(df, )

mes(m.1 = df$vr_m, m.2 = df$ctrl_m, sd.1 = df$vr_sd, sd.2 = df$ctrl_sd, n.1 = df$vr_n, 
    n.2 = df$ctrl_n,level = 95, dig = 4, verbose = TRUE, id=NULL, data=NULL)

df %>% mes(m.1 = vr_m, m.2 = ctrl_m, sd.1 = vr_sd, sd.2 = ctrl_sd, n.1 = vr_n, 
    n.2 = ctrl_n,level = 95, dig = 4, verbose = TRUE, id=NULL)

mes(m.1 = vr_m, m.2 = ctrl_m, sd.1 = vr_sd, sd.2 = ctrl_sd, n.1 = vr_n, 
    n.2 = ctrl_n,level = 95, dig = 4, verbose = TRUE, id=NULL, data=df)

# THIS BELOW IS CHAOS THAT DOESN'T WORK BUT IS TRYING TO DO SOMETHING!

lapply(mes(m.1 = df$ctrl_m, m.2 = df$vr_m, sd.1 = df$ctrl_sd, sd.2 = df$vr_sd, n.1 = df$ctrl_n, 
    n.2 = df$vr_n,level = 95, dig = 4, verbose = TRUE, id=NULL, data=NULL))

lapply(df, )

mes(m.1 = df$vr_m, m.2 = df$ctrl_m, sd.1 = df$vr_sd, sd.2 = df$ctrl_sd, n.1 = df$vr_n, 
    n.2 = df$ctrl_n,level = 95, dig = 4, verbose = TRUE, id=NULL, data=NULL)

df %>% mes(m.1 = vr_m, m.2 = ctrl_m, sd.1 = vr_sd, sd.2 = ctrl_sd, n.1 = vr_n, 
    n.2 = ctrl_n,level = 95, dig = 4, verbose = TRUE, id=NULL)

mes(m.1 = vr_m, m.2 = ctrl_m, sd.1 = vr_sd, sd.2 = ctrl_sd, n.1 = vr_n, 
    n.2 = ctrl_n,level = 95, dig = 4, verbose = TRUE, id=NULL, data=df)


```

# Interpretation of effect sizes

Cohen's f: small = .10, medium = .25, large = .40 {for ANOVAs} \newline
Cohen's d: small = .20, medium = .50, large = .80 {for t-tests} \newline
Cohen's h: small = .20, medium = .50, large = .80 {for proportions} \newline
partial eta-squared (np^2^): small = .01, medium = .06, large = .14 {for ANOVAs 
and MANOVAs}
Squared canoninical correlation (r^2^): small = .01--.09, medium = .09--.25, 
large > .25 {discriminant analyses} [We might not need this, depending on 
the decisions around the Rodriguez paper.] \newline
Vargha-Delaney's A {for Mann-Whitney U tests} 
* It produces a value between 0 and 1. 
+ When is exactly 0.5, the two techniques achieve equal performance. 
+ When A is less than 0.5, the first technique is worse. 
+ When A is more than 0.5, the second technique is worse. 
The closer to 0.5, the smaller the difference between the techniques; the farther from 
0.5, the larger the difference. 
http://doofussoftware.blogspot.com/2012/07/measuring-effect-size-with-vargha.html


# Table 1 
## Hua et al paper

The paper did not report the statistics from their comparisons, other than the
p-values. For each of the 10 tests, I used the summary statistics (means & standard 
deviation) reported in the paper to compute the required values. 

I conducted independent t-tests (I report the t-value and p-value) from 
the `BSDA` R package [@arnholt_bsda_2017]. I also used the `compute.es` R package 
[@re_computees_2020] to calculate the effect size (Cohen's d) and the 95% confidence 
intervals (CI) around this parameter. Small, medium, and large effect sizes 
correspond to d values of 0.2, 0.5, and 0.8, respectively (Cohen, 1988). 

```{r Hua_pain_before}

df <- df_hua %>% 
  filter(measure == "pain_before")

# CALCULATE THE EFFECT SIZE VALUES

# 1 is ctrl group
d1 <- mes(m.1 = ctrl_m, m.2 = vr_m, 
          sd.1 = ctrl_sd, sd.2 = vr_sd, 
          n.1 = ctrl_n, n.2 = vr_n,
          level = 95, dig = 4, verbose = TRUE, id=1, data=df)


# RUN T-Test
# x is ctrl group
t <- tsum.test(mean.x = df$ctrl_m, s.x = df$ctrl_sd, n.x = df$ctrl_n, 
          mean.y = df$vr_m, s.y = df$vr_sd, n.y = df$vr_n,
          alternative = "two.sided")

# add columns for the d-value & CI
df <- df %>% 
  dplyr::mutate (d = round_tidy(d1[1,5],2), 
                 lower.ci = round_tidy(d1[1,7],2), 
                 upper.ci = round_tidy(d1[1,8],2))

# add columns for the t-value & p-value
df <- df %>% 
  dplyr::mutate("t-value" = round_tidy(t$statistic, 2),
                "p-value" = round_tidy(t$p.value, 3))

# compare the original p-value reported in the paper & the p-value we calculated
df$original_p == df$`p-value`

# specific name for this analysis
df_hua1 <- df 
```

```{r Hua_pain_during}

df <- df_hua %>% 
  filter(measure == "pain_during")

# CALCULATE THE EFFECT SIZE VALUES

# ctrl as 1
d1 <- mes(m.1 = ctrl_m, m.2 = vr_m, 
          sd.1 = ctrl_sd, sd.2 = vr_sd, 
          n.1 = ctrl_n, n.2 = vr_n,
          level = 95, dig = 4, verbose = TRUE, id=1, data=df)


# RUN T-Test
# ctrl as x
t <- tsum.test(mean.x = df$ctrl_m, s.x = df$ctrl_sd, n.x = df$ctrl_n, 
          mean.y = df$vr_m, s.y = df$vr_sd, n.y = df$vr_n,
          alternative = "two.sided")

# add columns for the d-value & CI
df <- df %>% 
  dplyr::mutate (d = round_tidy(d1[1,5],2), 
                 lower.ci = round_tidy(d1[1,7],2), 
                 upper.ci = round_tidy(d1[1,8],2))

# add columns for the t-value & p-value
df <- df %>% 
  dplyr::mutate("t-value" = round_tidy(t$statistic, 2),
                "p-value" = round_tidy(t$p.value, 3))

# compare the original p-value reported in the paper & the p-value we calculated
df$original_p == df$`p-value`

# specific name for this analysis
df_hua2 <- df 

```

```{r Hua_pain_after}

df <- df_hua %>% 
  filter(measure == "pain_after")

# CALCULATE THE EFFECT SIZE VALUES

# ctrl as 1
d1 <- mes(m.1 = ctrl_m, m.2 = vr_m, 
          sd.1 = ctrl_sd, sd.2 = vr_sd, 
          n.1 = ctrl_n, n.2 = vr_n,
          level = 95, dig = 4, verbose = TRUE, id=1, data=df)


# RUN T-Test
# ctrl as x
t <- tsum.test(mean.x = df$ctrl_m, s.x = df$ctrl_sd, n.x = df$ctrl_n, 
          mean.y = df$vr_m, s.y = df$vr_sd, n.y = df$vr_n,
          alternative = "two.sided")

# add columns for the d-value & CI
df <- df %>% 
  dplyr::mutate (d = round_tidy(d1[1,5],2), 
                 lower.ci = round_tidy(d1[1,7],2), 
                 upper.ci = round_tidy(d1[1,8],2))

# add columns for the t-value & p-value
df <- df %>% 
  dplyr::mutate("t-value" = round_tidy(t$statistic, 2),
                "p-value" = round_tidy(t$p.value, 3))

# note that the p-value is different
df$notes <- "p-value in text is .034"

# compare the original p-value reported in the paper & the p-value we calculated
df$original_p == df$`p-value`

# make a note if they are different
df$notes <- "p-values differ, but conclusions unchanged"

# specific name for this analysis
df_hua3 <- df 

```

```{r Hua_pain_before_cg}

df <- df_hua %>% 
  filter(measure == "pain_before_cg")

# CALCULATE THE EFFECT SIZE VALUES

# ctrl as 1
d1 <- mes(m.1 = ctrl_m, m.2 = vr_m, 
          sd.1 = ctrl_sd, sd.2 = vr_sd, 
          n.1 = ctrl_n, n.2 = vr_n,
          level = 95, dig = 4, verbose = TRUE, id=1, data=df)


# RUN T-Test
# ctrl as x
t <- tsum.test(mean.x = df$ctrl_m, s.x = df$ctrl_sd, n.x = df$ctrl_n, 
          mean.y = df$vr_m, s.y = df$vr_sd, n.y = df$vr_n,
          alternative = "two.sided")

# add columns for the d-value & CI
df <- df %>% 
  dplyr::mutate (d = round_tidy(d1[1,5],2), 
                 lower.ci = round_tidy(d1[1,7],2), 
                 upper.ci = round_tidy(d1[1,8],2))

# add columns for the t-value & p-value
df <- df %>% 
  dplyr::mutate("t-value" = round_tidy(t$statistic, 2),
                "p-value" = round_tidy(t$p.value, 3))

# compare the original p-value reported in the paper & the p-value we calculated
df$original_p == df$`p-value`

# make a note if they are different
df$notes <- "p-values differ, but conclusions unchanged"

# specific name for this analysis
df_hua4 <- df 

```

```{r Hua_pain_during_cg}

df <- df_hua %>% 
  filter(measure == "pain_during_cg")

# CALCULATE THE EFFECT SIZE VALUES

# ctrl as 1
d1 <- mes(m.1 = ctrl_m, m.2 = vr_m, 
          sd.1 = ctrl_sd, sd.2 = vr_sd, 
          n.1 = ctrl_n, n.2 = vr_n,
          level = 95, dig = 4, verbose = TRUE, id=1, data=df)


# RUN T-Test
# ctrl as x
t <- tsum.test(mean.x = df$ctrl_m, s.x = df$ctrl_sd, n.x = df$ctrl_n, 
          mean.y = df$vr_m, s.y = df$vr_sd, n.y = df$vr_n,
          alternative = "two.sided")

# add columns for the d-value & CI
df <- df %>% 
  dplyr::mutate (d = round_tidy(d1[1,5],2), 
                 lower.ci = round_tidy(d1[1,7],2), 
                 upper.ci = round_tidy(d1[1,8],2))

# add columns for the t-value & p-value
df <- df %>% 
  dplyr::mutate("t-value" = round_tidy(t$statistic, 2),
                "p-value" = round_tidy(t$p.value, 3))

# compare the original p-value reported in the paper & the p-value we calculated
df$original_p == df$`p-value`

# specific name for this analysis
df_hua5 <- df 

```

```{r Hua_pain_after_cg}

df <- df_hua %>% 
  filter(measure == "pain_after_cg")

# CALCULATE THE EFFECT SIZE VALUES

# ctrl as 1
d1 <- mes(m.1 = ctrl_m, m.2 = vr_m, 
          sd.1 = ctrl_sd, sd.2 = vr_sd, 
          n.1 = ctrl_n, n.2 = vr_n,
          level = 95, dig = 4, verbose = TRUE, id=1, data=df)


# RUN T-Test
# ctrl as x
t <- tsum.test(mean.x = df$ctrl_m, s.x = df$ctrl_sd, n.x = df$ctrl_n, 
          mean.y = df$vr_m, s.y = df$vr_sd, n.y = df$vr_n,
          alternative = "two.sided")

# add columns for the d-value & CI
df <- df %>% 
  dplyr::mutate (d = round_tidy(d1[1,5],2), 
                 lower.ci = round_tidy(d1[1,7],2), 
                 upper.ci = round_tidy(d1[1,8],2))
# add columns for the t-value & p-value
df <- df %>% 
  dplyr::mutate("t-value" = round_tidy(t$statistic, 2),
                "p-value" = round_tidy(t$p.value, 3))

# compare the original p-value reported in the paper & the p-value we calculated
df$original_p == df$`p-value`

# specific name for this analysis
df_hua6 <- df 

```

```{r Hua_distress_before}

df <- df_hua %>% 
  filter(measure == "distress_before")

# CALCULATE THE EFFECT SIZE VALUES

# ctrl as 1
d1 <- mes(m.1 = ctrl_m, m.2 = vr_m, 
          sd.1 = ctrl_sd, sd.2 = vr_sd, 
          n.1 = ctrl_n, n.2 = vr_n,
          level = 95, dig = 4, verbose = TRUE, id=1, data=df)


# RUN T-Test
# ctrl as x
t <- tsum.test(mean.x = df$ctrl_m, s.x = df$ctrl_sd, n.x = df$ctrl_n, 
          mean.y = df$vr_m, s.y = df$vr_sd, n.y = df$vr_n,
          alternative = "two.sided")

# add columns for the d-value & CI
df <- df %>% 
  dplyr::mutate (d = round_tidy(d1[1,5],2), 
                 lower.ci = round_tidy(d1[1,7],2), 
                 upper.ci = round_tidy(d1[1,8],2))

# add columns for the t-value & p-value
df <- df %>% 
  dplyr::mutate("t-value" = round_tidy(t$statistic, 2),
                "p-value" = round_tidy(t$p.value, 3))

# compare the original p-value reported in the paper & the p-value we calculated
df$original_p == df$`p-value`

# specific name for this analysis
df_hua7 <- df 

```

```{r Hua_distress_during}

df <- df_hua %>% 
  filter(measure == "distress_during")

# CALCULATE THE EFFECT SIZE VALUES

# ctrl as 1
d1 <- mes(m.1 = ctrl_m, m.2 = vr_m, 
          sd.1 = ctrl_sd, sd.2 = vr_sd, 
          n.1 = ctrl_n, n.2 = vr_n,
          level = 95, dig = 4, verbose = TRUE, id=1, data=df)


# RUN T-Test
# ctrl as x
t <- tsum.test(mean.x = df$ctrl_m, s.x = df$ctrl_sd, n.x = df$ctrl_n, 
          mean.y = df$vr_m, s.y = df$vr_sd, n.y = df$vr_n,
          alternative = "two.sided")

# add columns for the d-value & CI
df <- df %>% 
  dplyr::mutate (d = round_tidy(d1[1,5],2), 
                 lower.ci = round_tidy(d1[1,7],2), 
                 upper.ci = round_tidy(d1[1,8],2))

# add columns for the t-value & p-value
df <- df %>% 
  dplyr::mutate("t-value" = round_tidy(t$statistic, 2),
                "p-value" = round_tidy(t$p.value, 3))

# compare the original p-value reported in the paper & the p-value we calculated
df$original_p == df$`p-value`

# specific name for this analysis
df_hua8 <- df 

```

```{r Hua_distress_after}

df <- df_hua %>% 
  filter(measure == "distress_after")

# CALCULATE THE EFFECT SIZE VALUES

# ctrl as 1
d1 <- mes(m.1 = ctrl_m, m.2 = vr_m, 
          sd.1 = ctrl_sd, sd.2 = vr_sd, 
          n.1 = ctrl_n, n.2 = vr_n,
          level = 95, dig = 4, verbose = TRUE, id=1, data=df)


# RUN T-Test
# ctrl as x
t <- tsum.test(mean.x = df$ctrl_m, s.x = df$ctrl_sd, n.x = df$ctrl_n, 
          mean.y = df$vr_m, s.y = df$vr_sd, n.y = df$vr_n,
          alternative = "two.sided")

# add columns for the d-value & CI
df <- df %>% 
  dplyr::mutate (d = round_tidy(d1[1,5],2), 
                 lower.ci = round_tidy(d1[1,7],2), 
                 upper.ci = round_tidy(d1[1,8],2))

# add columns for the t-value & p-value
df <- df %>% 
  dplyr::mutate("t-value" = round_tidy(t$statistic, 2),
                "p-value" = round_tidy(t$p.value, 3))

# compare the original p-value reported in the paper & the p-value we calculated
df$original_p == df$`p-value`

# specific name for this analysis
df_hua9 <- df 

```

```{r Hua_dressing_changes}

df <- df_hua %>% 
  filter(measure == "dressing_changes")

# CALCULATE THE EFFECT SIZE VALUES

# ctrl as 1
d1 <- mes(m.1 = ctrl_m, m.2 = vr_m, 
          sd.1 = ctrl_sd, sd.2 = vr_sd, 
          n.1 = ctrl_n, n.2 = vr_n,
          level = 95, dig = 4, verbose = TRUE, id=1, data=df)


# RUN T-Test
# ctrl as x
t <- tsum.test(mean.x = df$ctrl_m, s.x = df$ctrl_sd, n.x = df$ctrl_n, 
          mean.y = df$vr_m, s.y = df$vr_sd, n.y = df$vr_n,
          alternative = "two.sided")

# add columns for the d-value & CI
df <- df %>% 
  dplyr::mutate (d = round_tidy(d1[1,5],2), 
                 lower.ci = round_tidy(d1[1,7],2), 
                 upper.ci = round_tidy(d1[1,8],2))

# add columns for the t-value & p-value
df <- df %>% 
  dplyr::mutate("t-value" = round_tidy(t$statistic, 2),
                "p-value" = round_tidy(t$p.value, 3))

# compare the original p-value reported in the paper & the p-value we calculated
df$original_p == df$`p-value`

# specific name for this analysis
df_hua10 <- df 

```

```{r Hua_bind_tables}

df_bind <- bind_rows(df_hua1, df_hua2, df_hua3, df_hua4, df_hua5, df_hua6, 
                     df_hua7, df_hua8, df_hua9, df_hua10)
```


```{r Hua_concatenate_table}

# combine the d value with the upper and lower boundaries into a new column
# effect size is Cohen's d
df_bind$"Cohen's d" <- paste0(df_bind$d,
                              " [",
                              df_bind$lower.ci,
                              ", ",
                              df_bind$upper.ci, 
                              "]")


# combine the mean & SD for the two conditions
df_bind$vr_m_sd <- paste0(df_bind$vr_m, 
                          " (", 
                          df_bind$vr_sd, 
                          ")")
df_bind$ctrl_m_sd <- paste0(df_bind$ctrl_m, 
                            " (", 
                            df_bind$ctrl_sd, 
                            ")")

```


```{r Hua_clean_dataframe}

# remove the columns that are now redundant

df_hua_final <- df_bind %>% 
  select (-c(d, lower.ci, upper.ci, ctrl_m, vr_m, ctrl_sd, vr_sd))

# rearrange the columns we are keeping
df_hua_final <- df_hua_final %>% 
  relocate (vr_m_sd, .after = vr_n) %>% 
  relocate (ctrl_m_sd, .after = ctrl_n) %>% 
  relocate (original_p, .after = `p-value`) %>% 
  relocate (notes, .after = last_col())

```

```{r Hua_table,include=TRUE}

landscape(knitr::kable(df_hua_final, 
                   caption = "Hua et al. Paper", 
                   align = 'llccccccccl')) %>% 
            column_spec(column = 11, width = "5cm")

```

```{r Hua_write_data}

write_csv(df_hua_final, here::here("data_output", "hua_data_output.csv"))
```

## Jeffs et al paper - NEED TO DO

This isn't done. I need to figure out a way to calculate the SDs. I
have some potential solutions (links in code), but the best approach
will be to get what I presume is the standard error from Figure 2 
using the Enguage Digitizer Software (REF) and then calculate it from 
that. The paper reports an effect size, but I don't know what the 
reported measure of effect size is, so I need to confirm it with my 
own analysis. I don't think I can or should report the t-test analysis
because the illustrated results are based on estimates after
controlling for age, sex, state anxiety, opiod analgesic use,
treatment length, and pre-procedural pain. (see p 402).

I also need to compare the pre- and post-procedural pain, but again, 
this will be tricky because it's not clear whether that was based
on the adjusted means (which I have) or the actual means (which I 
don't have)...

We might have to say that we just couldn't calculate it, but let's 
see...

```{r jeffs_figure_out_data, eval=FALSE}

# need to calculate standard deviations

# the sample size is small (<100), so we need to use the t-distribution to 
  # calculate the SE value (https://www.medcalc.org/manual/t-distribution.php)

# SD = sqrt(N) x (upper limit - lower limit)/(2 x SE value)

SD_calc <- function(N, upper, lower, SE) {
  SD <- (sqrt(N)) x (upper - lower)/(2 * SE)
  print(SD)
  }

# VR
N <- 8
upper <- 


```

## Kipping et al paper

For the continuous measures, they analysed mean change scores such that they
subtracted the baseline measures from the removal and application scores for 
the outcome variables (self-reported pain, self-reported nausea, nurses' report 
of pain, caregivers' report of pain). For these measures, Table 
\ref{tab:kipping_table} reports the t-tests that we ran on the
summary stats and the computed effect sizes (Cohen's d). For these analyses, 
as in Hua, we used the R packages `BSDA` [@arnholt_bsda_2017] and 
`compute.es` [@re_computees_2020].

```{r kipping_data_setup}

# for some reason vr_m was parsed as a character, so change it to a number
  # this will add NAs to the dataframe because some of the cells are empty. 
  # So use supress warning 
df_kip$vr_m <- suppressWarnings(as.numeric(df_kip$vr_m))

```

```{r kipping_self_pain_removal}

df <- df_kip %>% 
  filter(measure == "self_pain_removal")

# CALCULATE THE EFFECT SIZE VALUES

# 1 is ctrl group
d1 <- mes(m.1 = ctrl_m, m.2 = vr_m, 
          sd.1 = ctrl_sd, sd.2 = vr_sd, 
          n.1 = ctrl_n, n.2 = vr_n,
          level = 95, dig = 4, verbose = TRUE, id=1, data=df)


# RUN T-Test
# x is ctrl group
t <- tsum.test(mean.x = df$ctrl_m, s.x = df$ctrl_sd, n.x = df$ctrl_n, 
          mean.y = df$vr_m, s.y = df$vr_sd, n.y = df$vr_n, var.equal = TRUE, 
          alternative = "two.sided")

# add columns for the d-value & CI
df <- df %>% 
  dplyr::mutate (d = round_tidy(d1[1,5],2), 
                 lower.ci = round_tidy(d1[1,7],2), 
                 upper.ci = round_tidy(d1[1,8],2))

# add columns for the t-value & p-value
df <- df %>% 
  dplyr::mutate("t-value" = round_tidy(t$statistic, 2),
                "p-value" = round_tidy(t$p.value, 3))

# compare the original p-value reported in the paper & the p-value we calculated
df$original_p == df$`p-value`

# make a note if they are different
df$notes <- "p-values differ, but conclusions unchanged"

# specific name for this analysis
df_kip1 <- df 
```

```{r kipping_self_pain_application}

df <- df_kip %>% 
  filter(measure == "self_pain_application")

# CALCULATE THE EFFECT SIZE VALUES

# 1 is ctrl group
d1 <- mes(m.1 = ctrl_m, m.2 = vr_m, 
          sd.1 = ctrl_sd, sd.2 = vr_sd, 
          n.1 = ctrl_n, n.2 = vr_n,
          level = 95, dig = 4, verbose = TRUE, id=1, data=df)

# RUN T-Test
# x is ctrl group
t <- tsum.test(mean.x = df$ctrl_m, s.x = df$ctrl_sd, n.x = df$ctrl_n, 
          mean.y = df$vr_m, s.y = df$vr_sd, n.y = df$vr_n,
          alternative = "two.sided", var.equal = TRUE)

# add columns for the d-value & CI
df <- df %>% 
  dplyr::mutate (d = round_tidy(d1[1,5],2), 
                 lower.ci = round_tidy(d1[1,7],2), 
                 upper.ci = round_tidy(d1[1,8],2))

# add columns for the t-value & p-value
df <- df %>% 
  dplyr::mutate("t-value" = round_tidy(t$statistic, 2),
                "p-value" = round_tidy(t$p.value, 3))

# compare the original p-value reported in the paper & the p-value we calculated
df$original_p == df$`p-value`

# make a note if they are different
df$notes <- "p-values differ, but conclusions unchanged"

# specific name for this analysis
df_kip2 <- df 
```


```{r kipping_self_nausea_removal}

df <- df_kip %>% 
  filter(measure == "self_nausea_removal")

# CALCULATE THE EFFECT SIZE VALUES

# 1 is ctrl group
d1 <- mes(m.1 = ctrl_m, m.2 = vr_m, 
          sd.1 = ctrl_sd, sd.2 = vr_sd, 
          n.1 = ctrl_n, n.2 = vr_n,
          level = 95, dig = 4, verbose = TRUE, id=1, data=df)


# RUN T-Test
# x is ctrl group
t <- tsum.test(mean.x = df$ctrl_m, s.x = df$ctrl_sd, n.x = df$ctrl_n, 
          mean.y = df$vr_m, s.y = df$vr_sd, n.y = df$vr_n,
          alternative = "two.sided", var.equal = TRUE)

# add columns for the d-value & CI
df <- df %>% 
  dplyr::mutate (d = round_tidy(d1[1,5],2), 
                 lower.ci = round_tidy(d1[1,7],2), 
                 upper.ci = round_tidy(d1[1,8],2))

# add columns for the t-value & p-value
df <- df %>% 
  dplyr::mutate("t-value" = round_tidy(t$statistic, 2),
                "p-value" = round_tidy(t$p.value, 3))

# compare the original p-value reported in the paper & the p-value we calculated
df$original_p == df$`p-value`

# make a note if they are different
df$notes <- "p-values differ, but conclusions unchanged"

# specific name for this analysis
df_kip3 <- df 
```

```{r kipping_self_nausea_application}

df <- df_kip %>% 
  filter(measure == "self_nausea_application")

# CALCULATE THE EFFECT SIZE VALUES

# 1 is ctrl group
d1 <- mes(m.1 = ctrl_m, m.2 = vr_m, 
          sd.1 = ctrl_sd, sd.2 = vr_sd, 
          n.1 = ctrl_n, n.2 = vr_n,
          level = 95, dig = 4, verbose = TRUE, id=1, data=df)

# RUN T-Test
# x is ctrl group
t <- tsum.test(mean.x = df$ctrl_m, s.x = df$ctrl_sd, n.x = df$ctrl_n, 
          mean.y = df$vr_m, s.y = df$vr_sd, n.y = df$vr_n,
          alternative = "two.sided", var.equal = TRUE)

# add columns for the d-value & CI
df <- df %>% 
  dplyr::mutate (d = round_tidy(d1[1,5],2), 
                 lower.ci = round_tidy(d1[1,7],2), 
                 upper.ci = round_tidy(d1[1,8],2))

# add columns for the t-value & p-value
df <- df %>% 
  dplyr::mutate("t-value" = round_tidy(t$statistic, 2),
                "p-value" = round_tidy(t$p.value, 3))

# compare the original p-value reported in the paper & the p-value we calculated
df$original_p == df$`p-value`

# make a note if they are different
df$notes <- "p-values differ, but conclusions unchanged"

# specific name for this analysis
df_kip4 <- df 
```

```{r kipping_nurse_pain_removal}

df <- df_kip %>% 
  filter(measure == "nurse_pain_removal")

# CALCULATE THE EFFECT SIZE VALUES

# 1 is ctrl group
d1 <- mes(m.1 = ctrl_m, m.2 = vr_m, 
          sd.1 = ctrl_sd, sd.2 = vr_sd, 
          n.1 = ctrl_n, n.2 = vr_n,
          level = 95, dig = 4, verbose = TRUE, id=1, data=df)


# RUN T-Test
# x is ctrl group
t <- tsum.test(mean.x = df$ctrl_m, s.x = df$ctrl_sd, n.x = df$ctrl_n, 
          mean.y = df$vr_m, s.y = df$vr_sd, n.y = df$vr_n,
          alternative = "two.sided", var.equal = TRUE)

# add columns for the d-value & CI
df <- df %>% 
  dplyr::mutate (d = round_tidy(d1[1,5],2), 
                 lower.ci = round_tidy(d1[1,7],2), 
                 upper.ci = round_tidy(d1[1,8],2))

# add columns for the t-value & p-value
df <- df %>% 
  dplyr::mutate("t-value" = round_tidy(t$statistic, 2),
                "p-value" = round_tidy(t$p.value, 3))

# compare the original p-value reported in the paper & the p-value we calculated
df$original_p == df$`p-value`

# specific name for this analysis
df_kip5 <- df 
```

```{r kipping_nurse_pain_application}

df <- df_kip %>% 
  filter(measure == "nurse_pain_application")

# CALCULATE THE EFFECT SIZE VALUES

# 1 is ctrl group
d1 <- mes(m.1 = ctrl_m, m.2 = vr_m, 
          sd.1 = ctrl_sd, sd.2 = vr_sd, 
          n.1 = ctrl_n, n.2 = vr_n,
          level = 95, dig = 4, verbose = TRUE, id=1, data=df)


# RUN T-Test
# x is ctrl group
t <- tsum.test(mean.x = df$ctrl_m, s.x = df$ctrl_sd, n.x = df$ctrl_n, 
          mean.y = df$vr_m, s.y = df$vr_sd, n.y = df$vr_n,
          alternative = "two.sided", var.equal = TRUE)

# add columns for the d-value & CI
df <- df %>% 
  dplyr::mutate (d = round_tidy(d1[1,5],2), 
                 lower.ci = round_tidy(d1[1,7],2), 
                 upper.ci = round_tidy(d1[1,8],2))

# add columns for the t-value & p-value
df <- df %>% 
  dplyr::mutate("t-value" = round_tidy(t$statistic, 2),
                "p-value" = round_tidy(t$p.value, 3))

# compare the original p-value reported in the paper & the p-value we calculated
df$original_p == df$`p-value`

# make a note if they are different
df$notes <- "p-values differ, but conclusions unchanged"

# specific name for this analysis
df_kip6 <- df 
```


```{r kipping_care_pain_removal}

df <- df_kip %>% 
  filter(measure == "care_pain_removal")

# CALCULATE THE EFFECT SIZE VALUES

# 1 is ctrl group
d1 <- mes(m.1 = ctrl_m, m.2 = vr_m, 
          sd.1 = ctrl_sd, sd.2 = vr_sd, 
          n.1 = ctrl_n, n.2 = vr_n,
          level = 95, dig = 4, verbose = TRUE, id=1, data=df)


# RUN T-Test
# x is ctrl group
t <- tsum.test(mean.x = df$ctrl_m, s.x = df$ctrl_sd, n.x = df$ctrl_n, 
          mean.y = df$vr_m, s.y = df$vr_sd, n.y = df$vr_n,
          alternative = "two.sided", var.equal = TRUE)

# add columns for the d-value & CI
df <- df %>% 
  dplyr::mutate (d = round_tidy(d1[1,5],2), 
                 lower.ci = round_tidy(d1[1,7],2), 
                 upper.ci = round_tidy(d1[1,8],2))

# add columns for the t-value & p-value
df <- df %>% 
  dplyr::mutate("t-value" = round_tidy(t$statistic, 2),
                "p-value" = round_tidy(t$p.value, 3))

# compare the original p-value reported in the paper & the p-value we calculated
df$original_p == df$`p-value`

# make a note if they are different
df$notes <- "p-values differ, but conclusions unchanged"

# specific name for this analysis
df_kip7 <- df 
```

```{r kipping_care_pain_application}

df <- df_kip %>% 
  filter(measure == "care_pain_application")

# CALCULATE THE EFFECT SIZE VALUES

# 1 is ctrl group
d1 <- mes(m.1 = ctrl_m, m.2 = vr_m, 
          sd.1 = ctrl_sd, sd.2 = vr_sd, 
          n.1 = ctrl_n, n.2 = vr_n,
          level = 95, dig = 4, verbose = TRUE, id=1, data=df)

# RUN T-Test
# x is ctrl group
t <- tsum.test(mean.x = df$ctrl_m, s.x = df$ctrl_sd, n.x = df$ctrl_n, 
          mean.y = df$vr_m, s.y = df$vr_sd, n.y = df$vr_n,
          alternative = "two.sided", var.equal = TRUE)

# add columns for the d-value & CI
df <- df %>% 
  dplyr::mutate (d = round_tidy(d1[1,5],2), 
                 lower.ci = round_tidy(d1[1,7],2), 
                 upper.ci = round_tidy(d1[1,8],2))

# add columns for the t-value & p-value
df <- df %>% 
  dplyr::mutate("t-value" = round_tidy(t$statistic, 2),
                "p-value" = round_tidy(t$p.value, 3))

# compare the original p-value reported in the paper & the p-value we calculated
df$original_p == df$`p-value`

# make a note if they are different
df$notes <-"p-values differ, but conclusions unchanged"

# specific name for this analysis
df_kip8 <- df 
```


```{r kipping_rescue_doses}

# select the relevant data
df <- df_kip %>% 
  filter(measure == "rescue_doses")

# calculate number in each condition that did not get a rescue dose

vr_no_dose <- df$vr_n-df$vr_dose_n
ctrl_no_dose <- df$ctrl_n-df$ctrl_dose_n

# build a dataframe with the relevant data

obsfreq_or <- matrix (c(df$vr_dose_n, vr_no_dose, df$ctrl_dose_n, ctrl_no_dose), 
                     nrow = 2, ncol = 2)

obsfreq <- matrix (c(df$ctrl_dose_n, ctrl_no_dose, df$vr_dose_n, vr_no_dose), 
                     nrow = 2, ncol = 2)

# run the stats
chi <- chisq.test(obsfreq)

# run the z-test of proportions

dose <- c(df$vr_dose_n,df$ctrl_dose_n)
total <- c(df$vr_n, df$ctrl_n)

# two-tailed z-test for proportions
# same result as normal chi square, as expected
res2 <- prop.test(x = dose, n = total, alternative = "two.sided")

# a one-sided test gets us the same value as in the paper
res1 <- prop.test(x = dose, n = total, alternative = "less")

# calculate Cohen's h as effect size with CIs using cohens_h function

prop_1 = (df$ctrl_dose_n/(df$ctrl_dose_n+ctrl_no_dose))
prop_2 = (df$vr_dose_n/(df$vr_dose_n+vr_no_dose))
n1 = df$ctrl_n
n2 = df$vr_n

h <- cohens_h(prop_1, prop_2, n1, n2)
h <- round(h, 2)
```

Unlike the other measures, the rescue doses outcome was binary data. Of the 20 
adolescents in the VR group, only 3 needed rescue doeses (15%). Of the 21 
adolescents in the SD group, 9 needed rescue doses (43%). In the paper, they 
reported running a chi-square test, which produced a significant result at p =
.05.

We tried to reproduce this analysis, but our chi-square test revealed a 
non-significant difference, `r apa(chi)`. 
A one-sided z-test of proportions produced a similar result to that of the original
paper, z = `r round(sqrt(res1$statistic),2)`, p = `r round(res1$p.value, 3)`,
although this p-value still exceeds the alpha level of .05.  Using
the R `pwr` package [@champely_pwr_2020], we calculated Cohen's h, a measure
of effect size for proportions that can be interpreted using the same rules of 
thumb as Cohen's d for small, medium, and large differences (Cohen, 1988). This
analysis revealed a medium-large effect size (h = `r h[1]` [`r h[2]`, 
`r h[3]`]) with a large 95% confidence interval. Taken together, these 
results suggest this finding might not be reliable. 

```{r kipping_bind_tables}

df_bind <- bind_rows(df_kip1, df_kip2, df_kip3, df_kip4, df_kip5, df_kip6, 
                     df_kip7, df_kip8)
```


```{r kipping_concatenate_table}

# combine the d value with the upper and lower boundaries into a new column
# effect size is Cohen's d
df_bind$"Cohen's d" <- paste0(df_bind$d,
                              " [",
                              df_bind$lower.ci,
                              ", ",
                              df_bind$upper.ci, 
                              "]")


# combine the mean & SD for the two conditions
df_bind$vr_m_sd <- paste0(df_bind$vr_m, 
                          " (", 
                          df_bind$vr_sd, 
                          ")")
df_bind$ctrl_m_sd <- paste0(df_bind$ctrl_m, 
                            " (", 
                            df_bind$ctrl_sd, 
                            ")")

```


```{r Kipping_clean_dataframe}

# remove the columns that are now redundant

df_kip_final <- df_bind %>% 
  select (-c(d, lower.ci, upper.ci, ctrl_m, vr_m, ctrl_sd, vr_sd, 
             ctrl_dose_n, vr_dose_n))

# rearrange the columns we are keeping
df_kip_final <- df_kip_final %>% 
  relocate (vr_m_sd, .after = vr_n) %>% 
  relocate (ctrl_m_sd, .after = ctrl_n) %>% 
  relocate (original_p, .after = `p-value`) %>% 
  relocate (notes, .after = last_col())

```

```{r kipping_table,include=TRUE}

landscape(knitr::kable(df_kip_final, 
                       caption = "Kipping et al. Paper", 
                       align = 'llccccccccl')) %>% 
            column_spec(column = 11, width = "3cm")

```

```{r kipping_write_data}

write_csv(df_kip_final, here::here("data_output", "kipping_data_output.csv"))
```

## Sil et al

They used a one-way within-subjects ANOVA to compare the
three conditions (baseline, traditional distraction, VR distraction) on log 
transformed pain tolerance scores.

They reported a large effect size (f) of .56 from the ANOVA, but they didn't 
report Cohen's d for the mean comparisons nor include the confidence interval 
around the f value.

I couldn't recreate their analyses from the summary data because it's a 
within-subjects design, so we should just report the t-values & p-values from 
the paper (p. 6). But, based on the summary statistics reported in the paper, 
I have used the R package `effsize` 
[@torchiano_effsize_2020] to calculate the effect size (Cohen's d) with 
confidence intervals for the three comparisons.

As discussed on Sept 22nd, we probably want to pull the `n` out of the table
because it's a within-subjects design.

```{r sil_pain_base_vs_trad}

df <- df_sil

# select comparison
df <- df %>% 
  filter(comparison == "base vs. trad")

# CALCULATE THE STATS

# change the order of m1 and m2 so we get a positive d value

d1 <- mes(m.1 = m.2, m.2 = m.1, sd.1 = sd.2, sd.2 = sd.1, 
          n.1 = n, n.2 = n, level = 95, dig = 4, verbose = TRUE, id=1, data=df)


# add columns for the d-value & CI
df <- df %>% 
  dplyr::mutate (d = round_tidy(d1[1,5],2), 
                 lower.ci = round_tidy(d1[1,7],2), 
                 upper.ci = round_tidy(d1[1,8],2))


# specific name for this analysis
df_sil1 <- df 

```

```{r sil_pain_base_vs_vr}

df <- df_sil

# select comparison
df <- df %>% 
  filter(comparison == "base vs. VR")

# CALCULATE THE STATS

# change the order of m1 and m2 so we get a positive d value

d1 <- mes(m.1 = m.2, m.2 = m.1, sd.1 = sd.2, sd.2 = sd.1, 
          n.1 = n, n.2 = n, level = 95, dig = 4, verbose = TRUE, id=1, data=df)


# add columns for the d-value & CI
df <- df %>% 
  dplyr::mutate (d = round_tidy(d1[1,5],2), 
                 lower.ci = round_tidy(d1[1,7],2), 
                 upper.ci = round_tidy(d1[1,8],2))


# specific name for this analysis
df_sil2 <- df 

```

```{r sil_pain_trad_vs_vr}

df <- df_sil

# select comparison
df <- df %>% 
  filter(comparison == "trad vs VR")

# CALCULATE THE STATS

d1 <- mes(m.1 = m.1, m.2 = m.2, sd.1 = sd.1, sd.2 = sd.2, 
          n.1 = n, n.2 = n, level = 95, dig = 4, verbose = TRUE, id=1, data=df)

# add columns for the d-value & CI
df <- df %>% 
  dplyr::mutate (d = round_tidy(d1[1,5],2), 
                 lower.ci = round_tidy(d1[1,7],2), 
                 upper.ci = round_tidy(d1[1,8],2))


# specific name for this analysis
df_sil3 <- df 

```

```{r sil_bind_tables}

df_bind <- bind_rows(df_sil1, df_sil2, df_sil3)
```

```{r sil_concatenate_table}

# combine the d value with the upper and lower boundaries into a new column
# effect size is Cohen's d
df_bind$"Cohen's d" <- paste0(df_bind$d,
                              " [",
                              df_bind$lower.ci,
                              ", ",
                              df_bind$upper.ci, 
                              "]")


# combine the mean & SD for the two conditions in each comparison
df_bind$m_sd_1 <- paste0(df_bind$m.1, 
                          " (", 
                          df_bind$sd.1, 
                          ")")
df_bind$m_sd_2 <- paste0(df_bind$m.2, 
                            " (", 
                            df_bind$sd.2, 
                            ")")

```

```{r sil_clean_dataframe}

# remove the columns that are now redundant

df_sil_final <- df_bind %>% 
  select (-c(condition, mean, sd, m.1, sd.1, m.2, sd.2, d, lower.ci, upper.ci))

# rearrange the columns we are keeping
df_sil_final <- df_sil_final %>% 
  relocate (n, .after = comparison) %>% 
  relocate (m_sd_1, .after = n) %>% 
  relocate (m_sd_2, .after = m_sd_1)

```

```{r sil_table,include=TRUE}

knitr::kable(df_sil_final, 
             caption = "Sil et al. Paper", 
              align = 'lllcccccc') %>% 
  column_spec(column = 7, width = "1.5cm") %>% 
   kable_styling(full_width = F, 
                latex_options = "HOLD_position")

```

```{r sil_write_data}

write_csv(df_sil_final, here::here("data_output", "sil_data_output.csv"))
```

## Dahlquist et al. (2010)

They used one-way within-subjects ANOVA to compare the three conditions (baseline, 
traditional distraction, VR distraction) on log 
transformed pain tolerance scores. 

They reported a large effect size (f) of .65 from the ANOVA, 
but they didn't report Cohen's d for the mean comparisons nor include the 
confidence interval around the f value.

I won't be able to recreate their analyses from the summary data because it's a 
within-subjects design, so we should just report the t-values & p-values from 
the paper (p. 622).  But, based on the summary statistics reported in the paper, 
I have used the R package `effsize` [@torchiano_effsize_2020] to calculate the effect size 
(Cohen's d) with confidence intervals for the three comparisons.

```{r dahl_2010_pain_base_vs_distraction}

df <- df_dahl

# select comparison
df <- df %>% 
  filter(comparison == "base vs. distraction")

# CALCULATE THE STATS

# change the order of m1 and m2 so we get a positive d value

d1 <- mes(m.1 = m.2, m.2 = m.1, sd.1 = sd.2, sd.2 = sd.1, 
          n.1 = n, n.2 = n, level = 95, dig = 4, verbose = TRUE, id=1, data=df)

# add columns for the d-value & CI
df <- df %>% 
  dplyr::mutate (d = round_tidy(d1[1,5],2), 
                 lower.ci = round_tidy(d1[1,7],2), 
                 upper.ci = round_tidy(d1[1,8],2))

# specific name for this analysis
df_dahl1 <- df 

```

```{r dahl_2010_pain_base_vs_distraction+helmet}

df <- df_dahl

# select comparison
df <- df %>% 
  filter(comparison == "base vs. distraction+helmet")

# CALCULATE THE STATS

# change the order of m1 and m2 so we get a positive d value

d1 <- mes(m.1 = m.2, m.2 = m.1, sd.1 = sd.2, sd.2 = sd.1, 
          n.1 = n, n.2 = n, level = 95, dig = 4, verbose = TRUE, id=1, data=df)

# add columns for the d-value & CI
df <- df %>% 
  dplyr::mutate (d = round_tidy(d1[1,5],2), 
                 lower.ci = round_tidy(d1[1,7],2), 
                 upper.ci = round_tidy(d1[1,8],2))


# specific name for this analysis
df_dahl2 <- df 

```

```{r dahl_2010_pain_distraction_vs_distraction+helmet}

df <- df_dahl

# select comparison
df <- df %>% 
  filter(comparison == "distraction vs. distraction+helmet")

# CALCULATE THE STATS

d1 <- mes(m.1 = m.1, m.2 = m.2, sd.1 = sd.1, sd.2 = sd.2, 
          n.1 = n, n.2 = n, level = 95, dig = 4, verbose = TRUE, id=1, data=df)

# add columns for the d-value & CI
df <- df %>% 
  dplyr::mutate (d = round_tidy(d1[1,5],2), 
                 lower.ci = round_tidy(d1[1,7],2), 
                 upper.ci = round_tidy(d1[1,8],2))


# specific name for this analysis
df_dahl3 <- df 

```

```{r dahl_2010_bind_tables}

df_bind <- bind_rows(df_dahl1, df_dahl2, df_dahl3)
```

```{r dahl_2010_concatenate_table}

# combine the d value with the upper and lower boundaries into a new column
# effect size is Cohen's d
df_bind$"Cohen's d" <- paste0(df_bind$d,
                              " [",
                              df_bind$lower.ci,
                              ", ",
                              df_bind$upper.ci, 
                              "]")

# combine the mean & SD for the two conditions in each comparison
df_bind$m_sd_1 <- paste0(df_bind$m.1, 
                          " (", 
                          df_bind$sd.1, 
                          ")")
df_bind$m_sd_2 <- paste0(df_bind$m.2, 
                            " (", 
                            df_bind$sd.2, 
                            ")")

```

```{r dahl_2010_clean_dataframe}

# remove the columns that are now redundant

df_dahl_final <- df_bind %>% 
  select (-c(condition, mean, sd, m.1, sd.1, m.2, sd.2, d, lower.ci, upper.ci))

# rearrange the columns we are keeping
df_dahl_final <- df_dahl_final %>% 
  relocate (n, .after = comparison) %>% 
  relocate (m_sd_1, .after = n) %>% 
  relocate (m_sd_2, .after = m_sd_1)

```

```{r dahl_2010_table,include=TRUE}

knitr::kable(df_dahl_final, 
             caption = "Dahl et al. (2010) Paper", 
              align = 'lllcccccc') %>% 
  column_spec(column = 7, width = "1.5cm") %>% 
  column_spec(column = 3, width = "3cm") %>% 
  column_spec(column = 9, width = "2cm") %>% 
   kable_styling(full_width = F, 
                latex_options = "HOLD_position")

```

```{r dahl_2010_write_data}

write_csv(df_dahl_final, here::here("data_output", "dahlquist_2010_data_output.csv"))
```

## Dahlquist et al. (2009)

They had two age groups: younger (6-10 years) and older (11-14 years). 

They ran a 2 x 3 mixed factorial ANOVA to compare the two age groups and 
the three experimental conditions (baseline, traditional distraction, VR 
distraction) on pain tolerance scores (no transformation reported).

Their ANOVA revealed main effects of experimental condition, F (2,78) = 7.84, 
p < .001, f = .62, and a significant age by experimental condition interaction
effect for pain tolerance, F (2,78) = 3.82, p < .05, f = .43. Both of these 
are large effects (they did not report the confidence intervals around the f value.)

*Decisions:* They didn't report the means for the experimental conditions, so I 
can't *easily* report the means or effect size for that comparison. 
There's probably a way for me to figure it out, but I'm not sure how much you 
care... Likewise, their significant comparisons were for younger vs older children
in the baseline and VR conditions. Given that we don't care about age effects, 
I've not reported the comparison of the baseline pain tolerance scores across
the two age groups (but I can, if you want it). The pain tolerance scores of 
the younger vs older children were not significantly different in the 
distraction-only condition, so they didn't report the standard deviation around 
the two means. If you want stats around that comparison, I can likely pull that '
info from Figure 1 (p. 580), but how much do you care about it?

*Action*: We decided that we don't need the stats for those comparisons. No 
additional action required. 

To compare the pain tolerance scores of younger and older children in the 
distraction+helmet condition, I used the the `BSDA` R package 
[@arnholt_bsda_2017] to compute the t-test statistic based on the summary
statistics reported in the paper. I also used the `compute.es` R package 
[@re_computees_2020] to calculate the effect size 
(Cohen's d) and the 95% confidence intervals (CI) around this parameter.

```{r dahl_2009_pain_base_vs_distraction}

df <- df_dahl_09

# select comparison
df <- df %>% 
  filter(comparison == "distraction+helmet: younger vs. older")

# CALCULATE THE STATS

# x is the older group
t <- tsum.test(mean.x = df$m.2, s.x = df$sd.2, n.x = df$n.2, 
          mean.y = df$m.1, s.y = df$sd.1, n.y = df$n.1,
          alternative = "two.sided", var.equal = TRUE)

# change the order of m1 and m2 so we get a positive d value

d1 <- mes(m.1 = m.2, m.2 = m.1, sd.1 = sd.2, sd.2 = sd.1, 
          n.1 = n, n.2 = n, level = 95, dig = 4, verbose = TRUE, id=1, data=df)

# add columns for the d-value & CI
df <- df %>% 
  dplyr::mutate (d = round_tidy(d1[1,5],2), 
                 lower.ci = round_tidy(d1[1,7],2), 
                 upper.ci = round_tidy(d1[1,8],2))

df <- df %>% 
  dplyr::mutate("t-value" = round_tidy(t$statistic, 2),
              "p-value" = round_tidy(t$p.value, 3))


# specific name for this analysis
df_dahl1 <- df 

```

```{r dahl_09_bind_tables}

# I don't really need this, but keeping it in case we add additional comparisons
df_bind <- bind_rows(df_dahl1)
```

```{r dahl_09_concatenate_table}

# combine the d value with the upper and lower boundaries into a new column
# effect size is Cohen's d
df_bind$"Cohen's d" <- paste0(df_bind$d,
                              " [",
                              df_bind$lower.ci,
                              ", ",
                              df_bind$upper.ci, 
                              "]")

# combine the mean & SD for the two conditions in each comparison
df_bind$m_sd_1 <- paste0(df_bind$m.1, 
                          " (", 
                          df_bind$sd.1, 
                          ")")
df_bind$m_sd_2 <- paste0(df_bind$m.2, 
                            " (", 
                            df_bind$sd.2, 
                            ")")

```

```{r dahl_clean_dataframe}

# remove the columns that are now redundant

df_dahl_final <- df_bind %>% 
  select (-c(condition, mean, sd, m.1, sd.1, m.2, 
             sd.2, d, lower.ci, upper.ci, n, `age group`))

# rearrange the columns we are keeping
df_dahl_final <- df_dahl_final %>% 
  relocate (m_sd_1, .after = n.1) %>% 
  relocate (m_sd_2, .after = n.2) %>% 
  relocate (original_p, .after = 'p-value')

```

```{r dahl_table,include=TRUE}

landscape(knitr::kable(df_dahl_final, 
             caption = "Dahl et al. (2009) Paper", 
              align = 'llllcccccccc')) %>% 
  column_spec(column = 3, width = "3cm") %>% 
  column_spec(column = 5, width = "1.5cm") %>% 
  column_spec(column = 7, width = "1.5cm") %>% 
   kable_styling(full_width = F, 
                latex_options = "HOLD_position")

```

```{r dahl_write_data}

write_csv(df_dahl_final, here::here("data_output", "dahlquist_2009_data_output.csv"))
```

# Table 2
## Diaz-Orueta article 

The convergent validity stats are presented in Table 3.There's no point in 
pulling that data into R because no manipulation is necessary. 
Paola, you can create a table with the info from Table 3 that you want to report?

*Decision*: They also compare the test scores for those children in treatment and those 
not under treatment (Table 5). Do you want any of that data in a table? I can probably 
calculate r as an effect size (z value divided by the number of observations), 
but I'll need to figure out how to calculate the CIs. The `rcompanion` package
might be useful for this. But, I won't put any effort in this unless you want it.
I won't put any effort in this unless you want it. BUT, you’ll also have to 
decide *what* you want. They report about 20 different comparisons—--do you want 
them all? 

*ACTION*: We decided not to report any of the stats from Table 5. Paola will 
pull the data from Table 3 re: the correlations between the two measures. 

https://rcompanion.org/handbook/F_04.html

## Nolin et al. (2016)

They want to report the correlations between the two measures as an
indication of convergent validity. There's no point in pulling that data into
R because no manipulation is necessary. Paola, you can create a table with the 
info from Table 1 (first three columns: correct response, commission, reaction
time). I suggest revising it a little so that ClinicaVR is on the row above the three 
measures, rather than in the same row (like Table 3 in Diaz-Orueta). 

*ACTION*: Paola will pull the data from Table 1 re: the correlations between the 
two measures. 

## Negut et al. (2016)

They used a mixed factorial design with two between-subject factors: test
condition (VC assessment vs. traditional CPT) and clinical status 
(children with ADHD [n = `r df_negut$adhd_n[1]`] vs. 
typically-developing children [n = `r df_negut$td_n[1]`]), and one 
within-subjects factor: test modality 
(with vs. without distractors). They ran MANCOVA with age and IQ as covariates. 
I used the  R package `effectsize` [@ben-shachar_effectsize_2020] to calculate 
partial eta-squared with 90% confidence intervals for these effects (but, I report
their V, F, and p values). 


```{r negut_data}

clin_eta <- F_to_eta2(f = 7.06, df = 4, df_error = 66, ci = .90)

inter_eta <- F_to_eta2(f = 1.60, df = 4, df_error = 66, ci = .90)

```


Two key findings emerged from the multivariate analysis:

1. A significant effect of clinical status, V = .30, F(4,66) = 7.06, p < .001, 
np^2^ = `r round_tidy(clin_eta$Eta_Sq_partial, 3)`, 90% CI 
[`r round(clin_eta$CI_low, 3)`, `r round(clin_eta$CI_high, 3)`]). 
I used the `BSDA` R package [@arnholt_bsda_2017] to conduct independent 
t-tests comparing the children with ADHD  with the typically-developing 
children on errors of commission, errors of omission, and total correct responses 
[**I cannot find the means for response time**]. I also used the `compute.es` R package 
[@re_computees_2020] to calculate the effect size (Cohen's d) and the 95% confidence 
intervals (CI) around this parameter. See Table \ref{tab:negut_table} for relevant
descriptive and inferential statistics. 

2. No significant interaction between test condition and clinical status
of the children, V = .08, F(4,66) = 1.60, p > .05, (np^2^ = 
`r round(inter_eta$Eta_Sq_partial, 3)` 90% CI [`r round_tidy(inter_eta$CI_low, 3)`, 
`r round(inter_eta$CI_high, 3)`]), which suggests that the children with ADHD 
and the typically-developing children's performance was similar regardless of 
the type of test used. 

```{r negut_commission}

df <- df_negut

# select measure
df <- df %>% 
  filter(measure == "commission errors")

# CALCULATE THE STATS

d1 <- mes(m.1 = adhd_m, m.2 = td_m, sd.1 = adhd_sd, sd.2 = td_sd, 
          n.1 = adhd_n, n.2 = td_n, level = 95, dig = 4, verbose = TRUE, id=1, data=df)

# RUN T-Test
# x is the adhd group

t <- tsum.test(mean.x = df$adhd_m, s.x = df$adhd_sd, n.x = df$adhd_n, 
          mean.y = df$td_m, s.y = df$td_sd, n.y = df$td_n,
          alternative = "two.sided")

# add columns for the d-value & CI
df <- df %>% 
  dplyr::mutate (d = round_tidy(d1[1,5],2), 
                 lower.ci = round_tidy(d1[1,7],2), 
                 upper.ci = round_tidy(d1[1,8],2))

# add columns for the t-value & p-value
df <- df %>% 
  dplyr::mutate("t-value" = round_tidy(t$statistic, 2),
                "p-value" = round_tidy(t$p.value, 3))


# compare the original t-value reported in the paper & the p-value we calculated
df$original_t == df$`t-value`

# compare the original p-value reported in the paper & the p-value we calculated
df$original_d == df$d

# specific name for this analysis
df_negut1 <- df 

```

```{r negut_omission}

df <- df_negut

# select measure
df <- df %>% 
  filter(measure == "omission errors")

# CALCULATE THE STATS

d1 <- mes(m.1 = adhd_m, m.2 = td_m, sd.1 = adhd_sd, sd.2 = td_sd, 
          n.1 = adhd_n, n.2 = td_n, level = 95, dig = 4, verbose = TRUE, id=1, data=df)

# RUN T-Test
# x is the adhd group

t <- tsum.test(mean.x = df$adhd_m, s.x = df$adhd_sd, n.x = df$adhd_n, 
          mean.y = df$td_m, s.y = df$td_sd, n.y = df$td_n,
          alternative = "two.sided")

# add columns for the d-value & CI
df <- df %>% 
  dplyr::mutate (d = round_tidy(d1[1,5],2), 
                 lower.ci = round_tidy(d1[1,7],2), 
                 upper.ci = round_tidy(d1[1,8],2))

# add columns for the t-value & p-value
df <- df %>% 
  dplyr::mutate("t-value" = round_tidy(t$statistic, 2),
                "p-value" = round_tidy(t$p.value, 3))


# compare the original t-value reported in the paper & the p-value we calculated
df$original_t == df$`t-value`

# compare the original p-value reported in the paper & the p-value we calculated
df$original_d == df$d

# specific name for this analysis
df_negut2 <- df 

```

```{r negut_total}

df <- df_negut

# select measure
df <- df %>% 
  filter(measure == "total correct responses")

# CALCULATE THE STATS

d1 <- mes(m.1 = adhd_m, m.2 = td_m, sd.1 = adhd_sd, sd.2 = td_sd, 
          n.1 = adhd_n, n.2 = td_n, level = 95, dig = 4, verbose = TRUE, id=1, data=df)

# RUN T-Test
# x is the adhd group

t <- tsum.test(mean.x = df$adhd_m, s.x = df$adhd_sd, n.x = df$adhd_n, 
          mean.y = df$td_m, s.y = df$td_sd, n.y = df$td_n,
          alternative = "two.sided")

# add columns for the d-value & CI
df <- df %>% 
  dplyr::mutate (d = round_tidy(d1[1,5],2), 
                 lower.ci = round_tidy(d1[1,7],2), 
                 upper.ci = round_tidy(d1[1,8],2))

# add columns for the t-value & p-value
df <- df %>% 
  dplyr::mutate("t-value" = round_tidy(t$statistic, 2),
                "p-value" = round_tidy(t$p.value, 3))


# compare the original t-value reported in the paper & the p-value we calculated
df$original_t == df$`t-value`

# compare the original p-value reported in the paper & the p-value we calculated
df$original_d == df$d

df$notes <- "t-value different than original, but conclusions unchanged"

# specific name for this analysis
df_negut3 <- df 

```


```{r negut_bind_tables}

df_bind <- bind_rows(df_negut1, df_negut2, df_negut3)
```

```{r negut_concatenate_table}

# combine the d value with the upper and lower boundaries into a new column
# effect size is Cohen's d
df_bind$"Cohen's d" <- paste0(df_bind$d,
                              " [",
                              df_bind$lower.ci,
                              ", ",
                              df_bind$upper.ci, 
                              "]")


# combine the mean & SD for the two conditions in each comparison
df_bind$m_sd_adhd <- paste0(df_bind$adhd_m, 
                          " (", 
                          df_bind$adhd_sd, 
                          ")")
df_bind$m_sd_td <- paste0(df_bind$td_m, 
                            " (", 
                            df_bind$td_sd, 
                            ")")

```

```{r negut_clean_dataframe}

# remove the columns that are now redundant

df_negut_final <- df_bind %>% 
  select (-c(scale, adhd_n, adhd_m, adhd_sd, td_n, td_m, td_sd, d, lower.ci, upper.ci))

# rearrange the columns we are keeping
df_negut_final <- df_negut_final %>% 
  relocate (m_sd_adhd, .after = measure) %>% 
  relocate (m_sd_td, .after = m_sd_adhd) %>% 
  relocate (notes, .after = last_col())

```

```{r negut_table,include=TRUE}

landscape(knitr::kable(df_negut_final, 
             caption = "Negut et al. (2016) Paper", 
              align = 'lcccccccl') %>% 
  column_spec(column = 9, width = "2cm") %>% 
   kable_styling(full_width = F, 
                latex_options = "HOLD_position"))

```

```{r negut_write_data}

write_csv(df_sil_final, here::here("data_output", "negut_data_output.csv"))
```




## Pollak et al. (2009)

Pollak et al used 3 types of CPT (within-subjects, counterbalanced): 

  + VR classroom with an embedded CPT (VR-CPT)
  + similar CPT (No VR-CPT)
  + TOVA on a flatscreen. 

Sample: 37 boys (20 boys with ADHD & a control group of 17 boys).

*Note*: They have a typo in Table 1. For the Errors of Omission measure, they 
report a mean of .29 and a standard deviation of 48 in the TOVA score for 
the control group. They must have missed the decimal place, so I've corrected it
in my table. 

Also, they report an "approximate Cohen's d values" with no explanation of why 
they are approximate. Given that, I'm not concerned about the consistent, but
minimal differences between the values they reported and those I calculated. 

```{r pollak_response_time_d}

df <- df_pol

# select measure
df <- df %>% 
  filter(measure == "reaction time")

# confirm the p-value for the comparison (group main effect)
p <- pf(q = df$`original_F`[1], df1 = 1, df2 = 34, lower.tail = FALSE)

# add to tibble 
df <- df %>% 
  dplyr::mutate (`exact_p` = round_tidy(p,2)) 

# CALCULATE THE EFFECT SIZE

# we can calculate d for each of the three test types simultaneously. 
# adhd group is 1st group [calculating it for the three tests, 
  # comparing ctrl vs ADHD groups]

d1 <-
  mes(
    m.1 = adhd_m,
    m.2 = ctrl_m,
    sd.1 = adhd_sd,
    sd.2 = ctrl_sd,
    n.1 = adhd_n,
    n.2 = ctrl_n,
    level = 95,
    dig = 4,
    verbose = TRUE,
    id = 1,
    data = df
  )

# add columns for the d-value & CI
df <- df %>% 
  dplyr::mutate (d = round_tidy(d1[,5],2), 
                 lower.ci = round_tidy(d1[,7],2), 
                 upper.ci = round_tidy(d1[,8],2))

# specific name for this analysis
df_pol_rt <- df 
```

```{r pollak_response_time_t_tests}

# CALCULATE THE STATS FOR EACH COMPARISON

# RUN T-Test for TOVA

df <- df_pol_rt %>% 
  filter(`test type` == "TOVA")

# x is adhd group
t <- tsum.test(mean.x = df$adhd_m, s.x = df$adhd_sd, n.x = df$adhd_n, 
          mean.y = df$ctrl_m, s.y = df$ctrl_sd, n.y = df$ctrl_n,
          alternative = "two.sided")

# add columns for the t-value & p-value
df <- df %>% 
  dplyr::mutate("t-value" = round_tidy(t$statistic, 2),
                "p-value" = round_tidy(t$p.value, 3))

# specific name for this analysis
df_pol1 <- df 

# RUN T-Test for No VR-CPT

df <- df_pol_rt %>% 
  filter(`test type` == "No VR-CPT")

# x is adhd group
t <- tsum.test(mean.x = df$adhd_m, s.x = df$adhd_sd, n.x = df$adhd_n, 
          mean.y = df$ctrl_m, s.y = df$ctrl_sd, n.y = df$ctrl_n,
          alternative = "two.sided")

# add columns for the t-value & p-value
df <- df %>% 
  dplyr::mutate("t-value" = round_tidy(t$statistic, 2),
                "p-value" = round_tidy(t$p.value, 3))

# specific name for this analysis
df_pol2 <- df 


# RUN T-Test for VR-CPT

df <- df_pol_rt %>% 
  filter(`test type` == "VR-CPT")

# x is adhd group
t <- tsum.test(mean.x = df$adhd_m, s.x = df$adhd_sd, n.x = df$adhd_n, 
          mean.y = df$ctrl_m, s.y = df$ctrl_sd, n.y = df$ctrl_n,
          alternative = "two.sided")

# add columns for the t-value & p-value
df <- df %>% 
  dplyr::mutate("t-value" = round_tidy(t$statistic, 2),
                "p-value" = round_tidy(t$p.value, 3))

# specific name for this analysis
df_pol3 <- df 

```

```{r pollak_variability_d}

df <- df_pol

# select measure
df <- df %>% 
  filter(measure == "variability of reaction time")

# confirm the p-value for the comparison (group main effect)
p <- pf(q = df$`original_F`[1], df1 = 1, df2 = 34, lower.tail = FALSE)

# add to tibble 
df <- df %>% 
  dplyr::mutate (`exact_p` = round_tidy(p,2)) 

# CALCULATE THE EFFECT SIZE

# we can calculate d for each of the three test types simultaneously. 
# adhd group is 1st group [calculating it for the three tests, 
  # comparing ctrl vs ADHD groups]

d1 <-
  mes(
    m.1 = adhd_m,
    m.2 = ctrl_m,
    sd.1 = adhd_sd,
    sd.2 = ctrl_sd,
    n.1 = adhd_n,
    n.2 = ctrl_n,
    level = 95,
    dig = 4,
    verbose = TRUE,
    id = 1,
    data = df
  )

# add columns for the d-value & CI
df <- df %>% 
  dplyr::mutate (d = round_tidy(d1[,5],2), 
                 lower.ci = round_tidy(d1[,7],2), 
                 upper.ci = round_tidy(d1[,8],2))

# specific name for this analysis
df_pol_var <- df 
```

```{r pollak_variability_t_tests}

# CALCULATE THE STATS FOR EACH COMPARISON

# RUN T-Test for TOVA

df <- df_pol_var %>% 
  filter(`test type` == "TOVA")

# x is adhd group
t <- tsum.test(mean.x = df$adhd_m, s.x = df$adhd_sd, n.x = df$adhd_n, 
          mean.y = df$ctrl_m, s.y = df$ctrl_sd, n.y = df$ctrl_n,
          alternative = "two.sided")

# add columns for the t-value & p-value
df <- df %>% 
  dplyr::mutate("t-value" = round_tidy(t$statistic, 2),
                "p-value" = round_tidy(t$p.value, 3))

# specific name for this analysis
df_pol4 <- df 

# RUN T-Test for No VR-CPT

df <- df_pol_var %>% 
  filter(`test type` == "No VR-CPT")

# x is adhd group
t <- tsum.test(mean.x = df$adhd_m, s.x = df$adhd_sd, n.x = df$adhd_n, 
          mean.y = df$ctrl_m, s.y = df$ctrl_sd, n.y = df$ctrl_n,
          alternative = "two.sided")

# add columns for the t-value & p-value
df <- df %>% 
  dplyr::mutate("t-value" = round_tidy(t$statistic, 2),
                "p-value" = round_tidy(t$p.value, 3))

# specific name for this analysis
df_pol5 <- df 


# RUN T-Test for VR-CPT

df <- df_pol_var %>% 
  filter(`test type` == "VR-CPT")

# x is adhd group
t <- tsum.test(mean.x = df$adhd_m, s.x = df$adhd_sd, n.x = df$adhd_n, 
          mean.y = df$ctrl_m, s.y = df$ctrl_sd, n.y = df$ctrl_n,
          alternative = "two.sided")

# add columns for the t-value & p-value
df <- df %>% 
  dplyr::mutate("t-value" = round_tidy(t$statistic, 2),
                "p-value" = round_tidy(t$p.value, 3))

# specific name for this analysis
df_pol6 <- df 

```


```{r pollak_omission_d}

df <- df_pol

# select measure
df <- df %>% 
  filter(measure == "errors of omission")

# confirm the p-value for the comparison (group main effect)
p <- pf(q = df$`original_F`[1], df1 = 1, df2 = 34, lower.tail = FALSE)

# add to tibble 
df <- df %>% 
  dplyr::mutate (`exact_p` = round_tidy(p,2)) 

# CALCULATE THE EFFECT SIZE

# we can calculate d for each of the three test types simultaneously. 
# adhd group is 1st group [calculating it for the three tests, 
  # comparing ctrl vs ADHD groups]

d1 <-
  mes(
    m.1 = adhd_m,
    m.2 = ctrl_m,
    sd.1 = adhd_sd,
    sd.2 = ctrl_sd,
    n.1 = adhd_n,
    n.2 = ctrl_n,
    level = 95,
    dig = 4,
    verbose = TRUE,
    id = 1,
    data = df
  )

# add columns for the d-value & CI
df <- df %>% 
  dplyr::mutate (d = round_tidy(d1[,5],2), 
                 lower.ci = round_tidy(d1[,7],2), 
                 upper.ci = round_tidy(d1[,8],2))

# specific name for this analysis
df_pol_om <- df 
```


```{r pollak_omission_t_tests}

# CALCULATE THE STATS FOR EACH COMPARISON

# RUN T-Test for TOVA

df <- df_pol_om %>% 
  filter(`test type` == "TOVA")

# x is adhd group
t <- tsum.test(mean.x = df$adhd_m, s.x = df$adhd_sd, n.x = df$adhd_n, 
          mean.y = df$ctrl_m, s.y = df$ctrl_sd, n.y = df$ctrl_n,
          alternative = "two.sided")

# add columns for the t-value & p-value
df <- df %>% 
  dplyr::mutate("t-value" = round_tidy(t$statistic, 2),
                "p-value" = round_tidy(t$p.value, 3))

# specific name for this analysis
df_pol7 <- df 

# RUN T-Test for No VR-CPT

df <- df_pol_om %>% 
  filter(`test type` == "No VR-CPT")

# x is adhd group
t <- tsum.test(mean.x = df$adhd_m, s.x = df$adhd_sd, n.x = df$adhd_n, 
          mean.y = df$ctrl_m, s.y = df$ctrl_sd, n.y = df$ctrl_n,
          alternative = "two.sided")

# add columns for the t-value & p-value
df <- df %>% 
  dplyr::mutate("t-value" = round_tidy(t$statistic, 2),
                "p-value" = round_tidy(t$p.value, 3))

# specific name for this analysis
df_pol8 <- df 


# RUN T-Test for VR-CPT

df <- df_pol_om %>% 
  filter(`test type` == "VR-CPT")

# x is adhd group
t <- tsum.test(mean.x = df$adhd_m, s.x = df$adhd_sd, n.x = df$adhd_n, 
          mean.y = df$ctrl_m, s.y = df$ctrl_sd, n.y = df$ctrl_n,
          alternative = "two.sided")

# add columns for the t-value & p-value
df <- df %>% 
  dplyr::mutate("t-value" = round_tidy(t$statistic, 2),
                "p-value" = round_tidy(t$p.value, 3))

# specific name for this analysis
df_pol9 <- df 

```

```{r pollak_commission_d}

df <- df_pol

# select measure
df <- df %>% 
  filter(measure == "errors of commission")

# confirm the p-value for the comparison (group main effect)
p <- pf(q = df$`original_F`[1], df1 = 1, df2 = 34, lower.tail = FALSE)

# add to tibble 
df <- df %>% 
  dplyr::mutate (`exact_p` = round_tidy(p,2)) 

# CALCULATE THE EFFECT SIZE

# we can calculate d for each of the three test types simultaneously. 
# adhd group is 1st group [calculating it for the three tests, 
  # comparing ctrl vs ADHD groups]

d1 <-
  mes(
    m.1 = adhd_m,
    m.2 = ctrl_m,
    sd.1 = adhd_sd,
    sd.2 = ctrl_sd,
    n.1 = adhd_n,
    n.2 = ctrl_n,
    level = 95,
    dig = 4,
    verbose = TRUE,
    id = 1,
    data = df
  )

# add columns for the d-value & CI
df <- df %>% 
  dplyr::mutate (d = round_tidy(d1[,5],2), 
                 lower.ci = round_tidy(d1[,7],2), 
                 upper.ci = round_tidy(d1[,8],2))

# specific name for this analysis
df_pol_com <- df 
```


```{r pollak_commission_t_tests}

# CALCULATE THE STATS FOR EACH COMPARISON

# RUN T-Test for TOVA

df <- df_pol_com %>% 
  filter(`test type` == "TOVA")

# x is adhd group
t <- tsum.test(mean.x = df$adhd_m, s.x = df$adhd_sd, n.x = df$adhd_n, 
          mean.y = df$ctrl_m, s.y = df$ctrl_sd, n.y = df$ctrl_n,
          alternative = "two.sided")

# add columns for the t-value & p-value
df <- df %>% 
  dplyr::mutate("t-value" = round_tidy(t$statistic, 2),
                "p-value" = round_tidy(t$p.value, 3))

# specific name for this analysis
df_pol10 <- df 

# RUN T-Test for No VR-CPT

df <- df_pol_com %>% 
  filter(`test type` == "No VR-CPT")

# x is adhd group
t <- tsum.test(mean.x = df$adhd_m, s.x = df$adhd_sd, n.x = df$adhd_n, 
          mean.y = df$ctrl_m, s.y = df$ctrl_sd, n.y = df$ctrl_n,
          alternative = "two.sided")

# add columns for the t-value & p-value
df <- df %>% 
  dplyr::mutate("t-value" = round_tidy(t$statistic, 2),
                "p-value" = round_tidy(t$p.value, 3))

# specific name for this analysis
df_pol11 <- df 


# RUN T-Test for VR-CPT

df <- df_pol_com %>% 
  filter(`test type` == "VR-CPT")

# x is adhd group
t <- tsum.test(mean.x = df$adhd_m, s.x = df$adhd_sd, n.x = df$adhd_n, 
          mean.y = df$ctrl_m, s.y = df$ctrl_sd, n.y = df$ctrl_n,
          alternative = "two.sided")

# add columns for the t-value & p-value
df <- df %>% 
  dplyr::mutate("t-value" = round_tidy(t$statistic, 2),
                "p-value" = round_tidy(t$p.value, 3))

# specific name for this analysis
df_pol12 <- df 

```



```{r pollak_bind_tables}

df_bind <- bind_rows(df_pol1, df_pol2, df_pol3, df_pol4, df_pol5, df_pol6,
                     df_pol7, df_pol8, df_pol9, df_pol10, df_pol11, df_pol12)


# compare the original d-value reported in the paper & the d-value we calculated
df_bind <- df_bind %>% 
  dplyr::mutate("d comparison" = original_d == `d`)

# make a note if they are different
df_bind$notes <- "d values differ, but conclusions unchanged"

```

```{r pollak_concatenate_table}

# combine the d value with the upper and lower boundaries into a new column
# effect size is Cohen's d
df_bind$"Cohen's d" <- paste0(df_bind$d,
                              " [",
                              df_bind$lower.ci,
                              ", ",
                              df_bind$upper.ci, 
                              "]")

# combine the mean & SD for the two conditions in each comparison
df_bind$m_sd_ctrl <- paste0(df_bind$ctrl_m, 
                          " (", 
                          df_bind$ctrl_sd, 
                          ")")
df_bind$m_sd_adhd <- paste0(df_bind$adhd_m, 
                            " (", 
                            df_bind$adhd_sd, 
                            ")")

```

```{r pollak_clean_dataframe}

# remove the columns that are now redundant

df_pol_final <- df_bind %>% 
  select (-c(ctrl_m, ctrl_sd, adhd_m, adhd_sd, d, lower.ci, upper.ci, exact_p, 
             original_F, original_p, `d comparison`))

# rearrange the columns we are keeping
df_pol_final <- df_pol_final %>% 
  relocate (m_sd_ctrl, .after = ctrl_n) %>% 
  relocate (m_sd_adhd, .after = adhd_n) %>% 
  relocate (`Cohen's d`, .after = m_sd_adhd)

```

```{r pollak_table,include=TRUE}

landscape(knitr::kable(df_pol_final, 
             caption = "Pollak et al. (2009) Paper", 
              align = 'lllccccccccl')) %>% 
  column_spec(column = 1, width = "1.5cm") %>% 
  column_spec(column = 8, width = "2cm") %>% 
  column_spec(column = 12, width = "3cm") 

```

```{r pollak_write_data}

write_csv(df_pol_final, here::here("data_output", "pollak_data_output.csv"))
```


## Rodriguez et al. (2018)

All comparisons include effect sizes, so Paola can simply include the information
that she wants to mention. 

No manipulation in R is necessary, though you will have to give some thought to 
what you want to include. We can't do a direct comparison of means across the 
VR (Aula Nesplora) test and the TOVA test because the values are interpreted 
differently (high scores on VR are indicitative of defict; high scores on the 
TOVA are indicative of good executive functioning). Based on your write up, 
I suggest that you should include the following in the text itself 
(or something along these lines): 

```{r Rod check stats}

#partial eta-squared for VR
vr_eta <- F_to_eta2(f = 4.93, df = 24, df_error = 429, ci = .90)

# Cohen's f for VR
vr_f <- F_to_f(f = 4.93, df = 24, df_error = 429, ci = .95)

#partial eta-squared for TOVA
tova_eta <- F_to_eta2(f = 1.18, df = 24, df_error = 465, ci = .90)

```

1. They conducted two MANCOVAs (one for VR & one for TOVA) to examine differences 
across the four groups (control & 3 ADHD presentation groups: Inattentive, 
Impulsive and Hyperactivity, Combined) on the dependent variables (omissions, 
commissions, response time, and variability) across the two halves of the tests. 
I used the  `effectsize` package in R [@ben-shachar_effectsize_2020] to calculate 
partial eta-squared with 90% confidence intervals for both models.

  + The MANCOVA for VR (Aula Nesplora) included age as a covariate. The 
  multivariate model was stastically significant (np^2^ = 
  `r round(vr_eta$Eta_Sq_partial, 3)` 
  90% CI [`r round(vr_eta$CI_low, 3)`, `r round(vr_eta$CI_high, 3)`]), with significant 
  univariate effects for all dependent varialbes (.057 $\geq$ np^2^s $\leq$ .228). 

  + The MANCOVA for TOVA included IQ as a covariate. The multivariate 
  model was not stastically significant (np^2^ = `r round(tova_eta$Eta_Sq_partial, 3)` 
  90% CI [`r round(tova_eta$CI_low, 3)`, `r round(tova_eta$CI_high, 3)`]). 
  
  + Taken together, these findings suggest that the Aula Nesplora was 
  able to discriminate between children with and without ADHD symptoms, whereas
  the TOVA was not able to discriminate between the two groups. 

2. Rodrigues et al also explored whether the dependent variables (omissions, 
commissions, response time, and variability; split by test half) provided by the 
two tests could correctly predict group membership. Both analyses were
significant, but the function with the variables from the VR test correctly
classified more of the sample. 

  + The discriminant function for the VR test (Aula Nesplora) correctly 
  classified 56.60% of the sample. 
  
  + The discriminant function for TOVA correctly classified 33.70% of the sample. 

## Bioulac et al. (2012) - in progress

Sample: 36 children; 20 with ADHD and 16 in control group

They used a within-subjects design so the physicians first assessed them with 
the CPT and, after 10 min, the test of the Virtual Classroom. 

*Actions*: 

  + I need to pull the data from the two figures. The figure has the 95% CI, but 
I then need to calculate the standard deviations. I think I can do that using 
info from Cochrane. [I think I'll need to do the same thing with Jeffs data]
  + I need to figure out what I can do re: calculating effect sizes for the data 
that used non-parametric tests. The standard one is Vargha & Delaney's A, but I 
need to figure out if I can calculate it from the summary stats. Possible approaches: 

    + `effsize` needs raw data (https://rdrr.io/cran/effsize/man/VD.A.html)
    + `rcompanion` needs raw data (https://rcompanion.org/handbook/F_04.html

*Solution*:

  + Using the Mann-Whitney U values reported on p. 517, I can calculate 
the effect size with the Vargha-Delaney's A (using formula 7 from
@ruscio_probability-based_2008). But, I cannot calculate confidence intervals
without the original data.

```{r bioulac_A}


df <- df_bio

# Calculate A and add it to the tibble

df <- df %>% mutate (A =
VD.A.sum(U = df$u, n1 = df$ctrl_n, n2 = df$adhd_n))

```


```{r bioulac_concatenate_table}

# combine the d value with the upper and lower boundaries into a new column
# effect size is Cohen's d
df_bind$"Cohen's d" <- paste0(df_bind$d,
                              " [",
                              df_bind$lower.ci,
                              ", ",
                              df_bind$upper.ci, 
                              "]")

# combine the mean & SD for the two conditions in each comparison
df_bind$m_sd_ctrl <- paste0(df_bind$ctrl_m, 
                          " (", 
                          df_bind$ctrl_sd, 
                          ")")
df_bind$m_sd_adhd <- paste0(df_bind$adhd_m, 
                            " (", 
                            df_bind$adhd_sd, 
                            ")")

```

```{r bioulac_clean_dataframe}

# remove the columns that are now redundant

df_bio_final <- df_bind %>% 
  select (-c(ctrl_m, ctrl_sd, adhd_m, adhd_sd, d, lower.ci, upper.ci, exact_p, 
             original_F, original_p, `d comparison`))

# rearrange the columns we are keeping
df_bio_final <- df_bio_final %>% 
  relocate (m_sd_ctrl, .after = ctrl_n) %>% 
  relocate (m_sd_adhd, .after = adhd_n) %>% 
  relocate (`Cohen's d`, .after = m_sd_adhd)

```

```{r bioulac_table,include=TRUE}

landscape(knitr::kable(df_bio_final, 
             caption = "Bioulac et al. (2012) Paper", 
              align = 'lllccccccccl')) %>% 
  column_spec(column = 1, width = "1.5cm") %>% 
  column_spec(column = 8, width = "2cm") %>% 
  column_spec(column = 12, width = "3cm") 

```

```{r bioulac_write_data}

write_csv(df_pol_final, here::here("data_output", "bioulac_data_output.csv"))
```



\newpage
# References